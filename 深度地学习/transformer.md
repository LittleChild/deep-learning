transformer计算注意力的方式，按照矩阵详解点积缩放注意力公式计算：
![image](https://user-images.githubusercontent.com/35659023/123114151-b059cc00-d471-11eb-9868-6f54982d635c.png)

这样是算两个词编码的结果，计算多次后，将所有的注意力concat拼接起来，再乘一个额外的矩阵，变成和输入的词编码一个维度，然后输入到下一个编器中去。。。


====================
TBD
